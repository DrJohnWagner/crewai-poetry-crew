# 7. Discussion (“1001 Things to Discuss”)
*(To be drafted…)*

---

A recurring theme across our experiments is the difficulty of enforcing precise textual constraints through natural-language prompts alone. The comma deletion in Section 5.4.2 illustrates how transformer models resolve incompatible demands by minimizing total loss: when faced with “preserve every character,” “preserve the wording,” and “perform a genuine structural transformation,” the model obeyed the two constraints with the highest semantic weight and violated the one with the lowest. This is not an idiosyncratic failure but a predictable consequence of transformer language priors, which treat punctuation as a soft constraint and structural fracture as a semantically meaningful event. As a result, even tightly engineered multi-agent systems can produce “pressure-authorized micro-mutations” at architectural fault lines, revealing how probabilistic language models negotiate conflicting objectives beneath the surface of persona-conditioned behavior.
